\documentclass{jlreq}
\usepackage{amsmath,physics2,derivative,bm}
\usephysicsmodule{ab}
\title{Gauss関数の最尤推定}
\author{}

\begin{document}
	\maketitle
	\section{確率密度関数と最尤推定}
		ある確率変数\(x\)がパラメーター\(\theta\)を持つ関数\(f(x;\theta)\)に従う時，
		この関数を確率密度関数という．
			\begin{equation}
				x \sim P(x;\theta)
			\end{equation}

		ここで，互いに独立な\(N\)個の要素を持つ変数\(x=(x_0,x_1,x_2\cdots x_n)\)
		を用意し，各々の値に関する確率密度関数の値の積を\(\theta\)の関数とするとき，
		これは\(\theta\)の尤度と呼ぶ．これは\(\theta\)の尤もらしさを示す．
			\begin{equation}
				L(\theta) = \prod_n^N P(x_n;\theta)
			\end{equation}
		この尤度も最大にするようなパラメーター\(\theta\)の探索を最尤推定と呼ぶ．
		具体的には，あるパラメーター不明の確率分布に従うデータを得た時，最もデータを
		よく表すパラメーターを探索することである．

		通常は簡単のため尤度を対数で表した対数尤度を用いる．
			\begin{equation}
				\log_eL(\theta)=\log_e\prod_n^NP(x_n;\theta)=
				\sum_{n}^N\log_eP(x_n;\theta)
			\end{equation}
		

	\section*{Gauss関数の最尤推定}
		正規分布の確率関数はGauss関数であり，以下である．
			\begin{equation}
				f(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}
				\exp\ab[{-\frac{(x-\mu)^2}{2\sigma^2}}] \label{eq:Gauss}
			\end{equation}
		個数\(N\)のデータ\(x=(x_0,x_1,x_2,\cdots,x_n)\)を用意すると尤度関数は以下と
		なる．
			\begin{equation}
				L(x|\theta) = \sum_{n=0}^N \frac{1}{\sqrt{2\pi\sigma^2}}
				\exp\ab[{-\frac{(x_n-\mu)^2}{2\sigma^2}}] \label{eq:likelihood}
			\end{equation}
		Gauss関数には2つのパラメーターが存在するため，2回に分けて
		各々のパラメーターを決定する必要がある．
		
		\subsection{パラメーター\(\mu\)についての最尤推定}
			\eqref{eq:likelihood}式を\(\mu\)で偏微分するのだが，
			それに先んじて簡単のため両辺に対数を取って式を変形する．
				\begin{align}
					\log_e L(x|\theta) &= \log_e \sum_{n=0}^N 
						\frac{1}{\sqrt{2\pi\sigma^2}}
						\exp\ab[{-\frac{(x_n-\mu)^2}{2\sigma^2}}]\\
					&= \sum_{n=0}^N\log_e\frac{1}{\sqrt{2\pi\sigma^2}} +
						\log_e e^{-\frac{(x_n-\mu)^2}{2\sigma^2}}\\
					&= \sum_{n=0}^N -\frac{1}{2}\log_e(2\pi\sigma^2) -
					\frac{(x_n-\mu)^2}{2\sigma^2}\\
					&= -\frac{N}{2}\log_e(2\pi\sigma^2) -
						\frac{1}{2\sigma^2}\sum_{n=0}^N(x_n-\mu)^2\label{eq:mu-trans}
				\end{align}
			\eqref{eq:mu-trans}式を\(\sigma\)で偏微分する．
				\begin{equation}
					\pdv{}{\mu}L(x|\theta) =
						-\frac{1}{\sigma}\sum_{n=0}^N(x_n-\mu)\label{eq:mu-dv}
				\end{equation}
			最後に\eqref{eq:mu-dv}式を0とし，\(\mu\)について解く．
				\begin{align}
					-\frac{1}{\sigma}\sum_{n=0}^N(x_n-\mu) &= 0\\
					-N\mu + \sum_{n=0}^Nx_n &= 0\\
					\mu &= \frac{1}{N}\sum_{n=0}^Nx_n \equiv \bar{x} \label{eq:fish}
				\end{align}
			\eqref{eq:fish}をみるとGauss関数のパラメーター\(\mu\)はデータ\(x\)の
			平均に等しいことがわかる．
			したがって\(\mu\)はデータの平均である．

		\subsection{パラメーター\(\sigma\)についての最尤推定}
			\(\mu\)の場合と同様にして，\eqref{eq:mu-trans}式を\(\sigma\)で
			偏微分する．対数に注意し，その引数を\(u\)とする．（合成関数の微分）
				\begin{align}
					\pdv{}{\sigma}\log_eL(x|\theta) &= 
						-\frac{N}{2}\odv{}{u}\log_eu\odv{}{\sigma}2\pi\sigma^2 -
						\pdv{}{\sigma}\frac{1}{2\sigma^2}\sum_{n=0}^N(x_n-\mu)^2\\
					&= -\frac{N}{2}\cdot\frac{1}{u}\cdot4\pi\sigma +
						\frac{1}{\sigma^3}\sum_{n=0}^N(x_n-\mu)^2\\
					&= -\frac{N}{2}\cdot\frac{1}{2\pi\sigma^2}\cdot4\pi\sigma + 
						\frac{1}{\sigma^3}\sum_{n=0}^N(x_n-\mu)^2\\
					&= -\frac{N}{\sigma} + 
						\frac{1}{\sigma^3}\sum_{n=0}^N(x_n-\mu)^2\\
					&= \frac{1}{\sigma}\ab\{-N+\frac{1}{\sigma^2}
						\sum_{n=0}^N(x_n-\mu)^2\}\label{eq:sigma-dv}
				\end{align}
			最後に\eqref{eq:sigma-dv}式を0とし，\(\sigma\)について解く．
				\begin{align}
					-\frac{1}{\sigma}\ab\{-N+
						\frac{1}{\sigma^2}\sum_{n=0}^N(x_n-\mu)^2\} &= 0\\
					-N+\frac{1}{\sigma^2}\sum_{n=0}^N(x_n-\mu)^2 &= 0\\
					N\sigma^2 &= \sum_{n=0}^N(x_n-\mu)^2\\
					\sigma^2 &= \frac{1}{N}\sum_{n=0}^N(x_n-\mu)^2　
						\label{eq:sigma-fish}
				\end{align}
			\eqref{eq:sigma-fish}式を見るとこれは分散の定義そのものである．従ってガ
			ウス関数のパラメーター\(\sigma\)はデータの分散である．

			以上2回の最尤推定により，Gauss関数のパラメーターを解析的に決定すること
			ができた．よって，適当なデータを用意し，
			その平均と分散とを\eqref{eq:Gauss}式に代入することでヒストグラムを
			Gauss関数で近似することができる．


		\section{勾配降下法}
			前節ではGauss関数の最尤推定を行って解析的にパラメーターを導いたが，
			関数によっては解析的にパラメーターを導くことが困難，あるいは不可能といっ
			た場合が存在する．こうした場合においては数値解析的にパラメーターを求める
			ことが可能である．
			この分野では目的関数を最小化することを目的とする．この最小化
			する目的関数を損失関数と呼ぶ．今回の場合においては，損失関数は対数尤尤度
			関数と関係を持つ．
				\begin{equation}
					E(\theta) \equiv -\log_eL(\theta)
				\end{equation}
			こうした微分可能な関数の数値解析において最も簡単な方法は勾配降下法と呼ば
			れるものである．これは次式のように，関数の勾配を利用して解を求める．
				\begin{equation}
					\theta_{t+1} = \theta_t - \gamma\nabla E(\theta_t)
				\end{equation}

			これをGauss関数に適用する．Gauss関数にはパラメーター\(\mu\)および
			\(\sigma\)が存在した．これをまとめてよって\(\bm{\theta}\)とする．
			勾配降下法のためには\(\bm{\theta}\)の初期値\(\bm{\theta_0}\)を与えねばな
			らない．これで準備は済んだので勾配降下法を行っていく．まずは\(\theta_1\)
			を求める．
				\begin{align}
					\bm{\theta_1} &= \bm{\theta_0} - \gamma\nabla E(\bm{\theta_0})\\
					&= \bm{\theta_0}-\gamma
						\begin{pmatrix}
							\pdv{E}{\mu}\\
							\pdv{E}{\sigma}
						\end{pmatrix}
				\end{align}
			次に\(\theta_2\)を求めるのだが，計算は全く同じで添字が
			変化するのみである．
				\begin{align}
					\bm{\theta_2} &= \bm{\theta_1} - \gamma\nabla E(\bm{\theta_1})\\
					&= \bm{\theta_1}-\gamma
						\begin{pmatrix}
							\pdv{E}{\mu}\\
							\pdv{E}{\sigma}
						\end{pmatrix}
				\end{align}
			このように適当な値に収束するまで微分を繰り返していく．

\end{document}
